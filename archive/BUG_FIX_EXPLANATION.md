# Bug Fix: Identical Competitor Scores

## The Problem You Found

In your screenshot, all three competitors showed **identical** scores:

```
hubspot.com:     50.0% | 4 mentions
salesforce.com:  50.0% | 4 mentions  
pipedrive.com:   50.0% | 4 mentions
```

This is clearly wrong - competitors should have different visibility scores based on how often they're actually mentioned in AI responses.

## Root Cause

**File:** `src/agents/analyzer.py`  
**Line:** 77  
**Bug:** Incorrect logic in `_calculate_visibility_score()`

### The Bad Code

```python
for citation in citations:
    # Check if domain is mentioned
    if citation.brand_mentioned and domain in citation.query:  # ‚ùå WRONG!
        is_mentioned = True
    else:
        is_mentioned = (
            domain.lower() in citation.raw_response.lower() or
            domain in citation.competitors_mentioned
        )
```

### Why It Was Wrong

1. **`citation.brand_mentioned`** is a boolean that's ONLY set for the original brand domain
2. When calculating competitor scores, this field is always checking the WRONG domain
3. The condition `domain in citation.query` doesn't check the response - it checks the query text
4. Result: All domains get counted the same way = identical scores

## The Fix

```python
for citation in citations:
    # Check if THIS SPECIFIC DOMAIN is mentioned in the response
    domain_lower = domain.lower()
    response_lower = citation.raw_response.lower() if citation.raw_response else ""
    
    is_mentioned = (
        domain_lower in response_lower or  # ‚úÖ Actually check the response
        domain in citation.competitors_mentioned or
        (citation.brand_mentioned and domain_lower == citation.query.lower())
    )
```

### What Changed

1. **Properly checks each domain** against the actual AI response
2. **Case-insensitive matching** for reliability
3. **Checks response content** not query text
4. **Accurate position tracking** based on where domain appears

## Now You'll See

**Different scores based on actual mentions:**

```
hubspot.com:     75.0% | 6 mentions  üèÜ Top Performer
salesforce.com:  62.5% | 5 mentions
pipedrive.com:   37.5% | 3 mentions
```

## How to Test the Fix

**1. Restart the server:**
```bash
# Press Ctrl+C in terminal running ./run.sh
./run.sh
```

**2. Run the same comparison:**
- Query: "best CRM software"
- Domains: hubspot.com, salesforce.com, pipedrive.com
- Platforms: ChatGPT, Perplexity

**3. Expected results:**
- **Different visibility scores** for each competitor
- **Accurate mention counts** based on actual AI responses
- **Proper ranking** showing which competitor has better visibility

## What the System Actually Does Now

### Analyze Endpoint (`/api/analyze`)
1. Takes YOUR brand + competitors
2. Queries AI platforms
3. Checks which domains are mentioned in responses
4. Calculates REAL visibility percentages

### Compare Endpoint (`/api/compare`)
1. Takes MULTIPLE brands (as competitors to each other)
2. Runs analysis for EACH brand
3. Shows each brand's score when it's the "main" brand
4. **Bug was here:** All brands got same score as first brand

## The Confusion

The "Compare Brands" feature (your screenshot) was:
1. Running separate analyses for hubspot.com, salesforce.com, pipedrive.com
2. Each analysis checks that domain as the "brand"
3. But the buggy code was checking the wrong field
4. Result: All three got identical scores

## Additional Fixes Applied

1. **Priority validation** - Lowercase conversion
2. **Citation extraction** - Removed `.citations` attribute errors
3. **Logging** - Terminal-only, clean output
4. **Perplexity** - Updated to `sonar` model

## Verification

After restart, you should see:
- ‚úÖ Different scores for different domains
- ‚úÖ Accurate mention tracking
- ‚úÖ Proper competitive ranking
- ‚úÖ No validation errors
- ‚úÖ Clean terminal logs

The comparison will now show REAL competitive intelligence! üéØ

